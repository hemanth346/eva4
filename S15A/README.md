## Dataset Info

Using
1. 100 images of `empty - office spaces, office reception lounges and office kitchen` as *background(bg)*
1. 100 images of `people` as *foreground(fg)*

scrapped online, created a dataset of

1. 400k fg_bg images
    - Generated by overlaying foreground images top of background

1. 400k fg_bg_mask images (single channel, grayscale)
    - by overlaying corresponding foreground mask at the same postion on black canvas of bg shape
  
1. 400k depth images (single channel, grayscale)
    - Generated using pretrained weights of a SOTA model [Paper - High Quality Monocular Depth Estimation via Transfer Learning](https://arxiv.org/abs/1812.11941 'https://arxiv.org/abs/1812.11941')

    - bg and fg are choosen keeping in mind the pretrained weights available for the mentioned model. Refer [repo](https://github.com/ialhashim/DenseDepth 'https://github.com/ialhashim/DenseDepth') for weights and model details


#### Dataset is available for download using below link

https://drive.google.com/drive/folders/1-CmUdFjWjBJ4XBFHII-EbvlRY_LTueYa

The link redirects to a google drive folder which has 2 folders
1. Dataset
1. Compressed_Dataset

With below tree structure

```
- Dataset
  -- inp_bg
  -- inp_fg
  -- inp_fg_masks
  -- depth_maps
    -- bg1
      -- image1
      -- image2
      ...
    -- bg2
      -- image1
      -- image2
      ...
    -- bg3
    ...
  -- fg_bg
    -- bg1
      -- image1
      -- image2
      ...
    -- bg2
      -- image1
      -- image2
      ...
    -- bg3
    ...
  -- fg_bg_masks
    -- bg1
      -- image1
      -- image2
      ...
    -- bg2
      -- image1
      -- image2
      ...
    -- bg3
    ...
  -- annotations.txt
```

```
- Compressed_Dataset
  -- depth_maps
    -- bg1.zip
    -- bg2.zip
    ...
  -- fg_bg
    -- bg1.zip
    -- bg2.zip
    ...
  -- fg_bg_masks
    -- bg1.zip
    -- bg2.zip
    ...
```

### Overview
###### bg
![bg_collage](https://github.com/hemanth346/eva4/blob/master/S15A/images/bg.png?raw=true 'https://github.com/hemanth346/eva4/blob/master/S15A/images/bg.png')

###### fg
![fg_collage](https://github.com/hemanth346/eva4/blob/master/S15A/images/fg.png?raw=true 'https://github.com/hemanth346/eva4/blob/master/S15A/images/fg.png')

###### fg_masks
![fg_masks_collage](https://github.com/hemanth346/eva4/blob/master/S15A/images/fg_masks.png?raw=true 'https://github.com/hemanth346/eva4/blob/master/S15A/images/fg_masks.png')

###### fg_bg
![fg_bg_collage](https://github.com/hemanth346/eva4/blob/master/S15A/images/fg_bg.png?raw=true 'https://github.com/hemanth346/eva4/blob/master/S15A/images/fg_bg.png')

###### fg_bg_masks
![fg_bg_masks_collage](https://github.com/hemanth346/eva4/blob/master/S15A/images/fg_bg_mask.png?raw=true 'https://github.com/hemanth346/eva4/blob/master/S15A/images/fg_bg_mask.png')
###### depth_maps

![depth_maps_collage](https://github.com/hemanth346/eva4/blob/master/S15A/images/fg_bg_depth.png 'https://github.com/hemanth346/eva4/blob/master/S15A/images/fg_bg_depth.png')

## Dataset statistics

Uncompressed size : 1.8 GB

Kinds of images : bg, fg, fg_masks, fg_bg, fg_bg_masks, depth_maps

1. bg
    - 100 images, resized to (224,224)

1. fg
    - 100 transparent images with alpha channel(RGBA)
    - resized to (100,70) (random resize can be can also be configured)

1. fg_masks
    - 100 masks for respective fg images, created using alpha channels.
    - resized to (100,70)

1. fg_bg
    - 400k jpeg images of shape (224, 224, 3)
      - 100(bg) * 100(fg) * 2(flip) * 20(random overlay of fg over bg)
      ```
      Mean: [0.6045, 0.5874, 0.5730]

      Std: [0.2815, 0.2813, 0.2814]
      ```

1. fg_bg_masks
  - 400k single channel jpeg images of shape (224, 224)
    - 100(bg) * 100(fg_masks) * 2(flip) * 20(overlay of fg_masks over black canvas)
      ```
      Mean: [0.0563]
      Std: [0.2252]
      ```

1. depth_maps
  - 400k single channel jpeg images of shape (224, 224)
    - depth predictions for respective fg_bg image
      ```
      Mean: [0.3743]
      Std: [0.1962]
      ```
---

### Creating Dataset:

- bg and transparent fg images were downloaded using image crawler.
  
  > Initially used GIMP to add alpha channel, remove background(fg) and create mask(fg_mask). But the process is not required as transparent images are readily availabe and masks can be created using PIL or Opencv.
  
- **Creating mask :** Used Opencv thresholding on alpha channel of fg image to create fg_mask for respective image
  
  > Opencv by default will not read alphachannel, we have to pass a flag to make sure alphachannels are read

      fg  =  cv2.imread(fg_img, -1) # flag  for cv2.IMREAD_UNCHANGED
      _,  fg_mask  =  cv2.threshold(im[:,  :,  3],  0,  255,  cv2.THRESH_BINARY)

- **Creating flip images :** Used Opencv method filp method to flip the images horizontally.

      fg_flip = cv2.flip(fg, flipCode=1)
      mask_flip = cv2.flip(mask, flipCode=1)

      flipcode = 0: flip vertically
      flipcode > 0: flip horizontally
      flipcode < 0: flip vertically and horizontally

  

- **Overlaying fg over bg :** Created a function which takes in bg and fg images and overlays them to create fg_bg and its mask fg_bg_mask as well. Overlay is done by playing with alpha values on each of the channel, retaining bg pixels in transparent part and where not transparent making bg pixels zero and retaining fg pixels.

> Below is the core functionality

    alpha_fg = fg[:, :, 3] / 255.0
    alpha_bg = 1.0 - alpha_fg

    # Not disturbing orignal images
    fg_bg = bg.copy()
    # creating canvas for mask, can use PIL new image as well
    black = bg * np.zeros(bg.shape, dtype=np.int8)

    for c in range(0, 3):
        fg_bg[y1:y2, x1:x2, c] = (alpha_fg * fg[:, :, c] +
                                  alpha_bg * fg_bg[y1:y2, x1:x2, c])


- **Creating Depth maps :** Modified authors repo to work for single image and created depth_maps for generated fg_bg images on the fly.

      def predict_single_file(img_path, model, minDepth=10, maxDepth=1000):
          # normalize
          images = np.clip(np.asarray(Image.open(img_path), dtype=float) / 255, 0, 1)
          
          # Support multiple RGBs, one RGB image, even grayscale
          if len(images.shape) < 3: image = np.stack((images,images,images), axis=2)
          if len(images.shape) < 4: image = images.reshape((1, images.shape[0], images.shape[1], images.shape[2]))
          
          # compute predictions, expects 4D image
          outputs = model.predict(image)
          
          # Put in expected range
          DepthNorm = maxDepth/outputs
          outputs =  np.clip(DepthNorm, minDepth, maxDepth) / maxDepth
          
          # standarsize and covert to 255 range
          rescaled = outputs[0][:, :, 0]
          rescaled = rescaled - np.min(rescaled)
          rescaled = rescaled / np.max(rescaled)
          rescaled = rescaled * 255

          return rescaled


Generating above said images for single background took around 4 mins. `i.e. ~ 4  minutes for 4000 images.`

I've faced issues with file write from colab to Drive which has taken away most of the time, since it won't throw any error only files will not be avialble in drive once colab runtime finished.




[Link to Dataset creation Notebook](https://github.com/hemanth346/eva4/blob/master/S15A/Dataset2_nocompression.ipynb 'https://github.com/hemanth346/eva4/blob/master/S15A/Dataset2_nocompression.ipynb')


[Link to Compressed_Dataset creation Notebook](https://github.com/hemanth346/eva4/blob/master/S15A/Dataset.ipynb 'https://github.com/hemanth346/eva4/blob/master/S15A/Dataset.ipynb')

> Note: I've used shutil to compress which seems to stop writing to drive after some time and requires a flush umount and force remount. Refer [colab - github issue](https://github.com/googlecolab/colabtools/issues/287 'https://github.com/googlecolab/colabtools/issues/287'). Another option that can be added is to write each image to ZipFile instance on the go. Not tried here.

### Custom Dataset class and DataLoader object

Since the data is to be read from colab, if reading files the i/o operations is crashing the colab runtime. Other option is to copy the data to colab disk and run from it, even can crash runtime due to i/o operations involved.

To overcome the runtime crashes, data is directly read from zip files using below dataset

```
class ZipData(Dataset):
    def __init__(self, data_dir, size=224):
        self.paths = []
        for file in os.listdir(data_dir):
            fname = os.path.join(Path(data_dir, file))
            if zipfile.is_zipfile(fname):
                self.paths += [x.filename for x in zipfile.ZipFile(fname).infolist()]

        self.transform = transforms.Compose([
            transforms.Resize((size, size)),
            transforms.ToTensor()
        ])

    def read_img_from_zip(self, zip_name, file_name, array=True):
        imgdata = zipfile.ZipFile(zip_name).read(file_name)
        img = Image.open(io.BytesIO(imgdata))
        # img = img.convert("RGB")
        
        # for abumentations
        if array:
            img = np.array(img)
            return img 

        # PIL image for pytorch transforms
        return img 
    
    def __len__(self):
        return len(self.paths)
    
    def __getitem__(self, index):
        bg = self.paths[index].split('_')[0]
        img = self.read_img_from_zip(os.path.join(data_dir, bg+'.zip'), self.paths[index], array=False) 
        return self.transform(img)

```

For bg, fg and fg masks since the images are just hundred, used below dataset class

```

class FolderData(Dataset):
    def __init__(self, data_dir, suffix='jpg', size=224):
        pattern = '*'
        if suffix:
            pattern = '*.'+suffix
        self.files = list(data_dir.rglob(pattern))
        self.transform = transforms.Compose([
            transforms.Resize((size, size)),
            transforms.ToTensor()
        ])
    def __len__(self):
        return len(self.files)
    
    def __getitem__(self, index):
        img = Image.open(self.files[index])
        return self.transform(img)

```


### Calculating Mean and std:
  Calculting mean is straight forward, count all corresponsing pixel values and divide by number of pixels.
  
  There are 2 approaches that are widely used to calculate std, for huge image datasets
  
    1. Averaging samples of the std from mini batches
    1. Iterate over dataset twice
   
   I've tried both and chose to iterate twice, since the values will be accurate here and since this is just one time process, the computation can be ignored. But if new data is being added regularly then this becomes not possible. Averaging over batches will be used there.

##### Method 1 (close to accurate)

To calculate the standard deviation by averaging samples of the std from mini batches. While very close to the true std, itâ€™s not calculated exactly and can be leveraged if time/computation limitations. But in production settings where new data is added on daily basis this will work.

    def get_batchwise_avg_mean_std(dataset, batch_size=50):
        print(len(dataset))
        loader = DataLoader(dataset, 
                          batch_size=batch_size, 
                          shuffle=True)
        mean = 0.
        std = 0.
        nb_samples = 0.
        for data in loader:
            batch_samples = data.size(0)
            data = data.view(batch_samples, data.size(1), -1)
            mean += data.mean(2).sum(0)
            std += data.std(2).sum(0)
            nb_samples += batch_samples

        mean /= nb_samples
        std /= nb_samples
        # return mean, std
        print(mean, std)


##### Method 2 :
  Calculate mean first and then calculate variance and std using the mean

    def get_mean(dataset, batch_size=50):
        mean = 0.0
        loader = DataLoader(dataset, 
                          batch_size=batch_size, 
                          shuffle=True)
        for images in loader:
            batch_size = images.size(0) 
            images = images.view(batch_size, images.size(1), -1)
            mean += images.mean(2).sum(0)
        mean = mean / len(loader.dataset)
        return mean

    def get_std(dataset, mean, batch_size=50):
        var = 0.0
        loader = DataLoader(dataset, 
                          batch_size=batch_size, 
                          shuffle=True)
        for images in loader:
            batch_samples = images.size(0)
            # convert into 3 flattened channels
            images = images.view(batch_samples, images.size(1), -1)
            # take mean for each of these channels, substract from image channels
            # square them and add across channels to get variance
            var += ((images - mean.unsqueeze(1))**2).sum([0,2])
        # square root over total pixels
        std = torch.sqrt(var / (len(dataset)*dataset[0].shape[1]*dataset[0].shape[2]))
        return std


[Link to notebook for calculating Data statistics](https://github.com/hemanth346/eva4/blob/master/S15A/Depth_DataStats.ipynb 'https://github.com/hemanth346/eva4/blob/master/S15A/Depth_DataStats.ipynb')
